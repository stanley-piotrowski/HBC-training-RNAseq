---
title: "Differential Gene Expression Analysis"
output:
  html_notebook:
    toc: true
    toc_float: true
    theme: cerulean
---

```{r setup, warning = FALSE, message = FALSE}
# Load libraries
library(tidyverse)
library(readxl)
library(janitor)
library(kableExtra)
library(DESeq2)
library(RColorBrewer)
library(pheatmap)
library(DEGreport)
library(ggrepel)
library(tximport)
library(ggtext)

# Set ggplot theme
my_theme <- theme(
  panel.background = element_rect(color = "black", fill = "white"), 
  panel.grid = element_blank(),
  plot.title = element_markdown(),
  plot.subtitle = element_markdown(),
  plot.caption = element_markdown()
)
```


## Part I: Getting Started

### 1. Workflow: raw data to counts

The basic overview of an RNA-seq experiment involves sample and library preparation, sequencing, sequencing data QC, and expression quantification, followed by differential gene expression and functional analyses.  

Some notes on each of these processes are highlighted below:

#### Sample and library preparation

  * The first step in sample and library preparation is RNA extraction, which is generally achieved by isolating total RNA from a sample and treating with DNAse to digest any genomic DNA.
  * From here, many protocols utilize a messenger RNA (mRNA) enrichment step, or a ribosomal RNA (rRNA) depletion step, as rRNA is much more abundant in many cell types and can dominate reads from RNA-seq experiments.
  * There are two general enrichment strategies: poly-A selection, which target the poly-A tails of mRNA molecules using oligo-dT primers conjugated to magnetic beads that can be used in pull-down assays; and rRNA removal via hybridization with engineered oligos conjugated to biotin beads, which are used to extract the rRNA and purify the remaining RNA.
  * There are advantages and disadvantages to both strategies: in the mRNA enrichment strategy via poly-A selection, the 3'-end of the transcripts may be targeted more, and this approach also fails to purify other RNA species of interest (e.g., micro RNAs, etc); however, if the focus is solely on protein-encoding genes, this may be an appropriate choice.
  * Additional considerations for both methods can be found on the [RNA-seq blog](https://www.rna-seqblog.com/ribo-depletion-in-rna-seq-which-ribosomal-rna-depletion-method-works-best/).
  * Following RNA extraction and target enrichment, the next critical step is to evaluate the quality of the purified RNA
  * This can be accomplished using a variety of approaches, including: ultraviolet (UV) light absorbance, as nucleic acids absorb light mainly at 260 nm, and the technique can be used to detect the amount of contaminating protein in the sample (280 nm), as well as organic compounds left over from the extraction (230 nm); fluorescence methods, which use dyes that bind to nucleic acids and known reference standards (e.g., Qubit); agarose gel electrophoresis (although this approach isn't used as frequently);  the Agilent BioAnalyzer, which uses capillary electrophoresis to generate a size distribution of nucleic acid present in the sample, as well as concentration; and finally, real-time quantitative PCR (RT-qPCR), but this method may require optimization and thus may not be preferred during initial QC steps prior to library preparation.
  * Next, purified RNAs must be fragmented for compatability with Illumina sequencing chemistry, and this is typically accomplished by adding RNAses for a specific amount of time or physical shearing (e.g., sonication).
  * The resulting fragments undergo a size-selection process, typically using magnetic beads (specific insert sizes can be purified using gel isolation).
  * Following size selection, an additional QC step can be performed using the Agilent BioAnalyzer to evaluate the distribution of fragment sizes (additional rounds of fragmentationa and BioAnalyzer QC may be necessary to obtain the desired size distribution).
  * Next, double-stranded complimentary DNA (cDNA) is generated in a reverse transcription reaction, sequencing adapters are ligated onto the cDNA fragments, and the resulting pool of molecules are used in a PCR amplification reaction to increase the amount of starting material.  
  * Once the final library has been size-selected and purified, RT-qPCR will be used to quantify the concentration of library fragments with Illumina adapter sequences (i.e., those fragments that will actually bind to the Illumina flow cell), and each library will be pooled (if multiple libraries will be multiplexed).
  * An overall schematic of the RNA-seq sample and library preparation procedure is presented below (photo courtesy of [RNA-seq blog](https://www.rna-seqblog.com/introduction-to-rna-sequencing-and-analysis/)).

<br>
<p align="center">
<img src="https://www.rna-seqblog.com/wp-content/uploads/2015/07/experiment.png"
alt="RNA-seq sample and library preparation workflow.">
</p>
<br>

#### Sequencing

  * Illumina uses a sequencing-by-synthesis technology to generate sequencing data for each molecule.
  * To start, the cDNA libraries are denatured with sodium hydroxide (NaOH) and hybridized to the complimentary oligos on the surface of the flow cell.
  * Upon hybridization, cluster generation begins by amplifying clonal fragments, such that (in patterned flow cells), each nanowell receives one starting molecule and subsequently generates a clonal library with enough DNA molecules present to generate a detectable signal. 
  * Following cluster generation, the sequencing portion of the workflow begins, and fluorescently-labeled nucleotides with terminator elements are added, one base at a time, over the entire flow cell.
  * After the incorporation of each base, a laser excites the fluorophore, leading to a characteristic emission signal that is captured by a camera.
  * Software translates the intensity of the signal into digital data in the form of a base call and a probability that the call was incorrect.  
  * The terminator is cleaved, washed away, and a new fluorescently-labeled nucleotide is washed over the flow cell. 
  * An overview of the sequencing-by-synthesis process is shown below (photo courtesy of [Harvard Chan Bioinformatics Core Training](https://hbctraining.github.io/Intro-to-ChIPseq/lessons/02_QC_FASTQC.html)), from starting material through image acquisition and base calling.

<br>
<p align="center">
<img src="https://hbctraining.github.io/Intro-to-ChIPseq/img/sbs_illumina.png"
alt="Overview of Illumina sequencing-by-synthesis process.">
</p>
<br>
  
#### Quality control of raw sequencing data (FastQC)

  * During each sequencing cycle, Illumina generates a BCL basecall file, which is then converted to a FASTQ file for subsequent analysis.
  * The FASTQ file contains information about the sequencer and the flow cell (e.g., flow cell ID, X and Y coordinates of the sample, etc.), the read sequence, a "+" sign, and quality information denoting the probability of an incorrect base call at a given position. 
  * An example FASTQ file is given below, detailing the various metadata in the FASTQ file header (image courtesy of [databricks](https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html))

<br>
<p align="center">
<img src="https://databricks.com/wp-content/uploads/2016/05/FASTQ-format-1024x202.png" alt="Example FASTQ file.">
</p>
<br>

  * FastQC is a tool for evaluating the quality of sequencing data, giving the user basic statistics such as the distribution of quality scores across the length of the reds, per base N content (i.e., no call), and the number and identity of overrepresented sequences that could be caused by contamination from the library preparation process (e.g., adapter dimer contamination).

#### Quantify expression

  * After evaluating the quality of sequence data and identifying any potential issues that could cause problems downstream, the next step is to quantify the expression of each transcript.
  * In other words, the goal is to identify which genes (or targets, more generally), each read is derived from.
  * Splice-aware alignment software exists to align RNA-seq reads to a reference genome (e.g., HISAT2 and STAR), as RNA-seq reads may span exon-exon junctions that cannot be mapped using short-read aligners common in DNA sequencing approaches.
  * Other alignment tools are so called "lightweight" in that they don't rely on direct alignment to a reference genome- some of the popular software programs in this cateogry are Salmon, Kallisto, and Sailfish.
  * [Salmon](https://combine-lab.github.io/salmon/) works by quasi-mapping RNA-seq reads to a reference transcriptome, essentially bypassing the computation of trying to align reads to each of several possible mapping positions in a reference genome and quantifying how well it matches to each position; instead, it just generates counts directly from the reference transcriptome.
  * [Sailfish](http://www.cs.cmu.edu/~ckingsf/software/sailfish/) uses a similar approach, instead using k-mers and a set of reference sequences (e.g., a reference transcriptome) to generate abundance estimates without aligning to a reference genome.
  * [Kallisto](https://pachterlab.github.io/kallisto/about) uses an approach called pseudoalignment, in which only the transcripts from which RNA-seq reads would have originated from are considered, without regard for how the reads and reference transcriptome align to generate abundance estimates.

####Quality control of aligned sequence reads (STAR/Qualimap)

  * Although for this tutorial we'll be using the abundance estimates generated from Salmon, it is still important to evaluate how well the RNA-seq reads map to a reference genome.
  * This can be accomplished using splice-aware aligners like STAR or HiSAT2, both of which can generate binary alignment map (BAM) files.
  * BAM files are the compressed version of the sequence alignment/map format file (SAM), which describes the alignment of reads to a reference with two sections: the header section, which contains metadata like the read name, read length, and the alignment method; and an alignment section, with data on the read name, sequence, quality, and additioanl alignment information (e.g., chromosome, start coordinate, and alignment quality).
  * We can use the tool Qualimap to provide summary information for each BAM file, similar to how FastQC is used to evaluate potential issues with sequencing data.
  * For example, a screenshot is given below demonstrating how Qualimap can be used to evaluate the distribution of coverage across a reference for a given BAM file.

<br>
<p align="center">
<img src="http://qualimap.conesalab.org/img/screenshot_bamqc_coverage.png" alt="Example screenshot of the coverage across reference using Qualimap">
</p>
<br>

#### Quality control: aggregating results with MultiQC 

  * [MultiQC](https://multiqc.info/) is a program that can be used to aggregate the results from various bioinformatics QC analyses (e.g., FastQC, Qualimap, alignment log files, etc.) for each sample into a single HTML report.
  * Briefly, the user will run MultiQC in an analysis directory containing all of the QC and log files and generate a single HTML report with all samples aggregated together to identify any patterns or outliers that may warrant investgiation before proceeding with subsequent analyses.
  * An example screenshot from a MultiQC HTML report shows an  plot where the user can interactively investigate the GC content per sequence. 

<br>
<p align="center">
<img src="https://multiqc.info/docs/images/toolbox_highlight.png" alt="Example screenshot of a MultiQC HTML report showing an interactive plot of the GC content per sequence.">
</p>
<br>

### 2. Experimental design considerations

This section highlights factors to consider when designing RNA-seq experiments, particularly with regard to replicates and achieving the desired sequencing depth and coverage for the desired anlyses.

Some notes for each section are presented below:

#### Replicates

  * Technical replicates are replicate samples obtained from the same specimen to measure technical variation associated with a particular process (e.g., taking three DNA samples from the same mouse to measure technical variation in an assay).
  
  * Biological replicates are different samples taken from the same condition with the goal of measuring biological variability (e.g., taking three DNA samples from separate mice to measure biological variation between samples with a given assay).
  
  * In this tutorial, the authors suggest that technical replicates are not necessary, but biological replicates are necessary in order to detect meaningful differences in the gene expression between treatments. 
  
  * Further, when considering trade-offs between sequencing depth and biological replicates, the authors demonstrate with published data that biological replicates will identify more differentially expressed genes relative to simply increasing the sequencing depth (i.e., the number of reads per sample).
  
  * However, there are other factors to consider when designing the experiment- e.g., if a goal is to measure lowly expression differentially expressed genes, generally a greater sequencing depth is needed than would be recommended for a more general approach.
  
  * Coverage, a term used to describe how many times (on average) each position in a genome has been sequenced, is not used in RNA-seq experiments because gene expression is highly variable.  
  
#### Confounding

  * Confounding refers to situations in which a researcher cannot distinguish between biological variation and technical variation (e.g., all male mice were used in a control group and all female mice were used in an experimental group).
  
#### Batch effects 

  * Batch effects refer to situations where you have additional confounding variables related to components of the workflow- for example, sequencing all samples from a particular species on one flow cell, and all samples from another species on a different flow cell, when trying to compare transcriptional activity at the same genes across species.
  
  * Gilan and Mizrahi-Man (2015; F1000 Research) highlighted that when reviewing mouse ENCODE comparative gene expression data between mouse and human, data tended to cluster more by species than by tissue type- however, it was noted that each species was sequenced separately, thus introducing bias in the form of a batch effect.  Below is a figure showing a principal components analysis and heatmap using hierchical clustering of the original data.
  
<p align="center">
<img src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4516019/bin/f1000research-4-7019-g0001.jpg"
alt = "PCA and heatmap of original comparative gene expression data between mouse and human from Gilan and Mizrahi-Man 2015 demonstrating the batch effect.">
</p>

  * After accounting for the batch effect (introduced by sequencing human and mouse tissue samples on the same sequencing lane as their respective species), the data clustered by tissue as expected.  Below is a figure showing the same analyses with the data after accounting for the batch effect.
  
<p align="center">
<img src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4516019/bin/f1000research-4-7019-g0002.jpg"
alt="PCA and heatmap of comparative gene expression data between mouse and human, after accounting for batch effects, from Gilan and Mizrahi-Man 2015.">
</p>

  * The authors recommend that if batch effects can't be avoided, researchers should exercise caution to avoid confounding in experimental group allocation.
  
  * Additionally, it is recommended to include the batch ID (e.g., a numeric value) in experimental metadata so that during normal QC steps, if potential batch effects are identified, they can be accounted for with linear regression or similar techniques.  
  
#### Exercise

1) Fill in a group value in the `rna_isolation` field in the Excel file from the training GitHub page (linked [here](https://hbctraining.github.io/DGE_workshop_salmon_online/lessons/experimental_planning_considerations.html)).  The idea is to: a) randomly allocate group treatment for RNA isolation; and b) give equal numbers of RNA isolations for each researcher.  For both of these tasks, we'll create a vector of possible assignments (e.g., `group1` through `group6`), repeat twice (there are 12 total samples), and then use `sample()` to create a vector of randomly drawn group assignments that we can use in the `rna_isolation` field, for example.

```{r part1-ex1}
# Create data directory
dir.create("./metadata")

# The file was downloaded manually from GitHub and moved into the metadata directory

# Import the Excel file and use the janitor package to tidy the field names
exp_design_table <- read_excel("./metadata/exp_design_table.xlsx") %>% 
  clean_names()

# Now we want to allocate the group assignments
# We'll define a vector of groups from 1 to 6, repeated twice, and then use sample() to generate a random assignment to allocate different samples to rna_isolation groups
groups <- c(paste("group", 1:6, sep = "")) %>% 
  rep(2) %>% 
  sample(size = 12, replace = FALSE)

# Now we'll do the same thing to assign rna_isolations for each researcher
researcher <- c("AB", "CD") %>% 
  rep(6) %>% 
  sample(size = 12, replace = FALSE)

# Add the random group and researcher assignments to the data frame
exp_design_table <- exp_design_table %>% 
  mutate(rna_isolation = groups, 
         researcher = researcher)
  
# Print out the results
exp_design_table %>% 
  kbl(caption = "Experimental design table after random assignment to one of six RNA isolation groups and one of two researchers to control for batch effects.") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

### 3. Intro to DGE/setting up DGE analysis

This section describes the basic workflow of an RNA-seq experiment, the objectives and motivations behind the experiment, and how to set up analyses to evaluate differences in gene expression.  

Notes for each section are presented below:

#### Different gene expression analysis

  * The basic workflow in this notebook is to take the pseudocounts generated from the different lightweight alignment programs (e.g., Salmon) and convert them to counts of genes, perform QC on these data through normalization and unsupervised clustering to detect potential batch effects (e.g., principal components analysis; PCA), and then perform the differential expression analyses.  
  
#### Review of the dataset

  * The dataset that will be used in this tutorial is from Kenny et al. 2014 (found [here](https://pubmed.ncbi.nlm.nih.gov/25464849/)) in which the authors compare the expression of MOV10 in three different groups: MOV10 over-expression, in which HEK293F (human epithelial kidney cells) were transfected with a MOV10 transgene; MOV10 knockdown, in which a short interfering RNA (siRNA) was used to knock out MOV10 expression; and in which siRNA created a MOV10 knockdown via non-specific knock out.  
  
  * Importantly, each replicate has three replicates.
  
  * MOV10 associates with the fragile X mental retardation protein (FMRP), a protein responsible for normal cognitive development, but which is absent in the brain of those with Fragile X syndrome.
  
  * The authors hypothesized that MOV10 and FMRP act in coordination to suppress translation of bound RNAs, which may be implicated in the disease. 
  
  * In this analysis, we are mainly trying to identify patterns gene expression depending on the gain or loss of function of MOV10, and if there are genes with similar expression patterns shared between treatments. 
  
  * The data for the analysis were downloaded from the Sequence Read Archive (SRA) and processed using the steps defined in the above sections.
  
#### Setting up

To set up for the differential expression analyses, we need to download the Salmon results from the full dataset from [this](https://hbctraining.github.io/DGE_workshop_salmon_online/lessons/01b_DGE_setup_and_overview.html) page by right-clicking and saving the zipped directory directly into the project directory.  We can then unzip the directory, which will create a new `data` directory with subdirectories for each sample.  Next, we'll need to download the annotation file to map the transcripts to the genes.  We can download this text file from of the HBC's GitHub repositories [here](https://github.com/hbctraining/DGE_workshop_salmon/blob/master/data/tx2gene_grch38_ens94.txt).

```{r salmon-data-import}
# Unzip the data.zip directory
unzip("./data.zip")

# Remove the unwanted directories and the old data.zip directory
unlink(c("__MACOSX/", "data.zip"),
       recursive = TRUE)

# Download the annotation file from the GitHub repository
download.file("https://raw.githubusercontent.com/hbctraining/DGE_workshop_salmon_online/master/data/tx2gene_grch38_ens94.txt", 
              destfile = "./metadata/tx2gene_grch28_ens94.txt")
```

Now that we've downloaded all of the Salmon output files and the annotation file, we need to load the libraries from CRAN and Bioconductor (see setup code chunk at the beginning of this document).  Note, any Bioconductor packages need to be installed using the command `Biocmanager::install(<packagename>)`. 

#### Loading data

Initially, we will be working with the `quant.sf` Salmon output files (detailed information regarding the file formats and other documentation can be found [here](https://salmon.readthedocs.io/en/latest/file_formats.html).  An example of one of the files is shown below.

```{r salmon-quantsf-example}
# Print an example of the Salmon quant.sf output file
read_tsv("./data/Mov10_oe_1.salmon/quant.sf") %>% 
  head() %>% 
  kbl(caption = "Example quant.sf Salmon output for one of the MOV10 over-expression replicate RNA-seq libraries") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

The `quant.sf` output is a tab-separated quantification file, where each row is a quantification record containing the following fields:

* `Name`: contains the name of the transcript from the database.  For example, in the table above, ENST00000456328 is DEAD/H-box helicase 11 like 1 (more information on this particular transcript found [here](http://m.ensembl.org/Homo_sapiens/Transcript/Summary?db=core;g=ENSG00000223972;r=1:11869-14409;t=ENST00000456328)).

* `Length`: length of the transcript, in nucleotides.

* `EffectiveLength`: the effective length of the transcript, which takes into account the different factors being modeled like GC-content and the distribution of the fragment lengths from the underlying transcript that will influence the probability of sampling fragments from this target transcript.  

* `TPM`: transcripts per million, a relative metric of transcript abundance.

* `NumReads`: Salmon's estimate of the number of reads mapping to the particular transcript.  

In order to get these data in the right format for analysis with DESeq2, we need to convert the normalized TPM counts to non-normalized count estimates.  Additionally, we need to collapse our read counts per transcript to read counts per gene, since there are potentially several target transcripts per gene.  The first thing we need to do is name all of the `quant.sf` files to discriminate between samples in downstream analyses.  We'll first create two variables: one variable defining the paths for all directories containing data (there will be nine- one for each sample); and another vector containing the filenames for all of the `quant.sf` files.  We'll use these vectors to create unique names for each `quant.sf` file.

```{r rename-quant-files}
# Create a vector of the paths to the sample-specific directories containing data
samples <- list.files(path = "./data", 
                      full.names = T, 
                      pattern = "salmon$") # define the names of the directories as anything that ends with "salmon"

# Check to see that the list of directories is correct
samples # All of the sample-specific directories are present

# Create a vector of all of the quant.sf file names for each sample-specific directory
files <- file.path(samples, "quant.sf")

# Check to see that the paths to all of the quant.sf files are correct
files # All of the quant.sf file paths are correct

# Create unique names for each of the directories 
names(files) <- str_replace(samples, "./data/", "") %>% 
  str_replace(".salmon", "")

# The above code assigns names to each of the elements in the files vector
# First, it takes the sample-specific directory names and removes the "./data/" prefix, then removes the ".salmon" extension to create a unique name that can be referred to easily in downstream analyses
```

Now, we'll need to import our annotation file that contains the reference information linking the transcript IDs that were used to estimate read counts with Salmon to the genes that the transcripts are derived from.

```{r import-annotation-file}
# Import the annotation file
tx2gene <- read_tsv("./metadata/tx2gene_grch28_ens94.txt")

# Take a look at the annotation file
tx2gene %>%
  head() %>% 
  kbl(caption = "Annotation file with reference information linking the transcript ID used in the Salmon index to its gene ID and symbol") %>%
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()
  
```

The first column is the transcript ID, the second column is the gene ID, and the third ID is the gene symbol. Now we'll need to run the `tximport()` function, which imports the transcript-level estimates from different programs (e.g., Salmon) and summarizes abundances to the gene level using the provided annotation file.  There are several arguments: `files`, which we'll define using the `files` vector containing all of the directories to the `quant.sf` files (defined above); `type`, to define which program was used to generate the counts; `tx2gene`, which specifies the two-column data frame linking the transcript ID to the gene ID (i.e., the first two columns in the annotation file); and `countsFromAbundance`, which we'll set to `lengtScaledTPM` to we use the counts from the `TPM` field in the `quant.sf` file, because we don't want to use the counts that are correlated with the effective length.

```{r run-tximport}
# Run tximport for all files
txi <- tximport(files = files, 
                type = "salmon", 
                tx2gene = select(tx2gene, c("tx_id", "ensgene")),
                countsFromAbundance = "lengthScaledTPM")

```

#### Viewing data

The resulting `txi` object is a list of various matrices: `abundance`, `counts`, `length`, and `countsFromAbundance` (this is just the character given in the `countsFromAbundance` argument in the function call).    

```{r txi-attributes}
attributes(txi)
```

Now we want to look specifically at the `counts` matrix, which we'll round to the nearest whole number and then convert to a data frame.

```{r txi-counts-dataframe}
# Round the txi counts data and conver the matrix to a data frame
counts_data <- txi$counts %>% 
  round() %>% 
  data.frame() %>% 
  rownames_to_column("Gene")

# Take a look at the data frame
counts_data %>% 
  head() %>% 
  kbl(caption = "Count data for each gene obtained from Salmon pseudocounts") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

# Save this counts table to a file in the data directory
write_csv(counts_data, 
          file = "./data/counts_data.csv")
```

#### Creating metadata

Now we just want to create a metadata file linking the headers from the `txi$count` matrix to the sample groups (i.e., control, overexpression, and knockdown).  The Salmon data file that was downloaded from the tutorial webpage actually only had two MOV10 knockdown replicates, so we will need to change the metadata data frame accordingly.  

```{r}
# Create a metadata table
counts_metadata <- data.frame(sample_type = c(rep("control", 3), 
                           rep("MOV10_knockdown", 2), 
                           rep("MOV10_overexpression", 3)), 
           sample_name = colnames(txi$counts))

# View the metadata
counts_metadata %>% 
  kbl(caption = "Metadata linking the sample type to the sample name in the gene counts table") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

# Write this data frame to a file in the metadata directory
write_csv(counts_metadata, 
          file = "./metadata/counts_metadata.csv")
```

## Part II: QC and setting up for DESeq2

### 1. RNA-seq counts distribution

#### Explore RNA-seq count data

 Below is the counts data frame with the genes in the first column and the samples in the following columns.  Each cell contains an integer which represents the number of transcripts from a particular gene in that sample.
 
```{r visualize-count-data}
counts_data %>% 
  head() %>% 
  kbl(caption = "Count data for each gene obtained from Salmon pseudocounts") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()
```
 
We can plot the distribution of counts for each sample using a histogram, which will demonstrate the common features of RNA-seq data: there are generally low counts for a large proportion of the genes, and a large tail in the distribution as there is no upper limit to gene expression.

```{r counts-histogram}
# Generate a histogram for each sample
counts_data %>% 
  pivot_longer(cols = !Gene, 
               names_to = "Sample", 
               values_to = "Count") %>% 
  ggplot(aes(Count)) +
  geom_histogram(stat = "bin", 
                 bins = 200) +
  facet_wrap(~ Sample) +
  my_theme

```

#### Modeling count data

* Count data can be modeled either with the binomial distribution (e.g., modeling the number of times you would expect to see heads in a series of coin tosses) or the Poisson distribution.  The latter if often used when the number of trials is very large, but assumes that the mean and the variance are equal.  

* To evaluate whether we meet this assumption, we'll use the three MOV10 over-expression replicates and generate vectors for the mean counts for each gene and the variance.  

* Then, we can visualize the relationship between the mean and variance in a scatterplot.

```{r mean-variance-mov10-overexpression, warning = FALSE, message = FALSE}
# Calculate mean and variance for each gene in the three Mov10 overexpression samples
counts_data_stats <- counts_data %>% 
  select(Gene, contains("Mov10_oe")) %>% 
  mutate(Mean_count = apply(select(., contains("Mov10_oe")), 1, mean), 
         Variance = apply(select(., contains("Mov10_oe")), 1, var))

# Plot the mean and variance and add a continuous slope line where mean = variance
counts_data_stats %>% 
  ggplot(aes(Mean_count, Variance)) +
  geom_point(alpha = 0.5) +
  scale_x_log10(limits = c(1, 1e9)) +
  scale_y_log10(limits = c(1, 1e9)) +
  geom_abline(intercept = 0, 
              slope = 1, 
              linetype = "dashed", 
              color = "red") +
  my_theme +
  labs(x = "Mean counts", 
       y = "Variance counts", 
       title = "Mean versus variance in RNA-seq count data in MOV10 over-expression samples")
```

* This plot reveals that the mean and variance are not equal- if they were, the points would be right along the dashed red line.

* Instead, we can see that for genes with higher mean counts, the variance is higher than the mean (i.e., the points are above the dashed line).

* The genes with lower mean counts also display heteroscedasticity, or the tendency for low mean counts to have high variability in the variance estimates.

#### An alternative: the negative binomial distribution

* [This blogpost](http://bridgeslab.sph.umich.edu/posts/why-do-we-use-the-negative-binomial-distribution-for-rnaseq) provides a brief explanation for using the negative binomial distribution to model count data in RNA-seq data sets.

* In summary, RNA-seq data cannot be normally distributed, because we cannot have negative counts, or fractions of counts; all of the counts must be positive whole numbers (i.e., positive integers).  

* Because the Poisson distribution assumes the mean and variance are equal, this is not appropriate for RNA-seq data sets where the number of replicates is relatively small and thus, we cannot accurately estimate variance.

* The negative binomial distribution is an appropriate alternative, as this probability distribution doesn't assume equal mean and variance for count data. 

* However, if we had enough replicates (which is often unfeasible for RNA-seq experiments), we may be able to use the Poisson if the variance is reduced such that the assumption of equal mean and variance is satisfied.

#### Exercise

1) If we observed little to no variability between samples, this might suggest that we can model the RNA-seq count data with the Poisson distribution where mean and variance are equal.  

2) We would expect the mean and variance to be equal for this dataset.  

#### Replicates and variability

* Recall, biological replicates come from the same biological condition but different sources (e.g., DNA samples taken from mice raised in the same experimental conditions).

* One of the challenges in differential gene expression analysis is identifying and correcting for unknown sources of biological (e.g., mice with different genetic backgrounds) or technical variation (e.g., different lot numbers of reagents, etc.).

* HBC recommends running running experiments in triplicate (e.g., three replicates for a experimental group and three replicates for a control group).

* However, more replicates are generally always better because we can get more precise estimates of the mean and variance, and thus, better detect meaningful differences between groups.

* The authors also include a figure from a study showing the increase in the number of differentially expressed genes as a function of the number of replicates included in a study, more so than the sequencing depth.  

#### Differential Expression with DESeq2

* We will be using the DESeq2 package for differential gene expression analysis, which employs the negative binomial distribution to model counts from RNA-seq data.

### 2. Count normalization

### 3. Sample-level QC

### 4. Design formulas

### 5. Hypothesis testing and multiple test correction

## Part III: DESeq2

### 1. Description of steps for DESeq2

### 2. Wald test results

### 3. Summarizing results and extracting significant gene lists

### 4. Visualization

### 5. Likelihood Ratio Test Results

### 6. Time Course Analysis

## Part IV: Functional Analysis

### 1. Gene Annotation

### 2. Functional Analysis I: Over-Representation Analysis

### 3. Functional Analysis II: Functional Class Scoring/GSEA  

