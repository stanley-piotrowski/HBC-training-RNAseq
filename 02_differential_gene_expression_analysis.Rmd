---
title: "Differential Gene Expression Analysis"
output:
  html_notebook:
    toc: true
    toc_float: true
    theme: cerulean
---

```{r setup, warning = FALSE, message = FALSE}
# Load libraries
library(tidyverse)
library(readxl)
library(janitor)
library(kableExtra)
library(DESeq2)
library(RColorBrewer)
library(pheatmap)
library(DEGreport)
library(ggrepel)
library(tximport)
library(ggtext)
library(patchwork)

# Set ggplot theme
my_theme <- theme(
  panel.background = element_rect(color = "black", fill = "white"), 
  panel.grid = element_blank(),
  plot.title = element_markdown(),
  plot.subtitle = element_markdown(),
  plot.caption = element_markdown()
)
```


## Part I: Getting Started

### 1. Workflow: raw data to counts

The basic overview of an RNA-seq experiment involves sample and library preparation, sequencing, sequencing data QC, and expression quantification, followed by differential gene expression and functional analyses.  

Some notes on each of these processes are highlighted below:

#### Sample and library preparation

  * The first step in sample and library preparation is RNA extraction, which is generally achieved by isolating total RNA from a sample and treating with DNAse to digest any genomic DNA.
  * From here, many protocols utilize a messenger RNA (mRNA) enrichment step, or a ribosomal RNA (rRNA) depletion step, as rRNA is much more abundant in many cell types and can dominate reads from RNA-seq experiments.
  * There are two general enrichment strategies: poly-A selection, which target the poly-A tails of mRNA molecules using oligo-dT primers conjugated to magnetic beads that can be used in pull-down assays; and rRNA removal via hybridization with engineered oligos conjugated to biotin beads, which are used to extract the rRNA and purify the remaining RNA.
  * There are advantages and disadvantages to both strategies: in the mRNA enrichment strategy via poly-A selection, the 3'-end of the transcripts may be targeted more, and this approach also fails to purify other RNA species of interest (e.g., micro RNAs, etc); however, if the focus is solely on protein-encoding genes, this may be an appropriate choice.
  * Additional considerations for both methods can be found on the [RNA-seq blog](https://www.rna-seqblog.com/ribo-depletion-in-rna-seq-which-ribosomal-rna-depletion-method-works-best/).
  * Following RNA extraction and target enrichment, the next critical step is to evaluate the quality of the purified RNA
  * This can be accomplished using a variety of approaches, including: ultraviolet (UV) light absorbance, as nucleic acids absorb light mainly at 260 nm, and the technique can be used to detect the amount of contaminating protein in the sample (280 nm), as well as organic compounds left over from the extraction (230 nm); fluorescence methods, which use dyes that bind to nucleic acids and known reference standards (e.g., Qubit); agarose gel electrophoresis (although this approach isn't used as frequently);  the Agilent BioAnalyzer, which uses capillary electrophoresis to generate a size distribution of nucleic acid present in the sample, as well as concentration; and finally, real-time quantitative PCR (RT-qPCR), but this method may require optimization and thus may not be preferred during initial QC steps prior to library preparation.
  * Next, purified RNAs must be fragmented for compatability with Illumina sequencing chemistry, and this is typically accomplished by adding RNAses for a specific amount of time or physical shearing (e.g., sonication).
  * The resulting fragments undergo a size-selection process, typically using magnetic beads (specific insert sizes can be purified using gel isolation).
  * Following size selection, an additional QC step can be performed using the Agilent BioAnalyzer to evaluate the distribution of fragment sizes (additional rounds of fragmentationa and BioAnalyzer QC may be necessary to obtain the desired size distribution).
  * Next, double-stranded complimentary DNA (cDNA) is generated in a reverse transcription reaction, sequencing adapters are ligated onto the cDNA fragments, and the resulting pool of molecules are used in a PCR amplification reaction to increase the amount of starting material.  
  * Once the final library has been size-selected and purified, RT-qPCR will be used to quantify the concentration of library fragments with Illumina adapter sequences (i.e., those fragments that will actually bind to the Illumina flow cell), and each library will be pooled (if multiple libraries will be multiplexed).
  * An overall schematic of the RNA-seq sample and library preparation procedure is presented below (photo courtesy of [RNA-seq blog](https://www.rna-seqblog.com/introduction-to-rna-sequencing-and-analysis/)).

<br>
<p align="center">
<img src="https://www.rna-seqblog.com/wp-content/uploads/2015/07/experiment.png"
alt="RNA-seq sample and library preparation workflow.">
</p>
<br>

#### Sequencing

  * Illumina uses a sequencing-by-synthesis technology to generate sequencing data for each molecule.
  * To start, the cDNA libraries are denatured with sodium hydroxide (NaOH) and hybridized to the complimentary oligos on the surface of the flow cell.
  * Upon hybridization, cluster generation begins by amplifying clonal fragments, such that (in patterned flow cells), each nanowell receives one starting molecule and subsequently generates a clonal library with enough DNA molecules present to generate a detectable signal. 
  * Following cluster generation, the sequencing portion of the workflow begins, and fluorescently-labeled nucleotides with terminator elements are added, one base at a time, over the entire flow cell.
  * After the incorporation of each base, a laser excites the fluorophore, leading to a characteristic emission signal that is captured by a camera.
  * Software translates the intensity of the signal into digital data in the form of a base call and a probability that the call was incorrect.  
  * The terminator is cleaved, washed away, and a new fluorescently-labeled nucleotide is washed over the flow cell. 
  * An overview of the sequencing-by-synthesis process is shown below (photo courtesy of [Harvard Chan Bioinformatics Core Training](https://hbctraining.github.io/Intro-to-ChIPseq/lessons/02_QC_FASTQC.html)), from starting material through image acquisition and base calling.

<br>
<p align="center">
<img src="https://hbctraining.github.io/Intro-to-ChIPseq/img/sbs_illumina.png"
alt="Overview of Illumina sequencing-by-synthesis process.">
</p>
<br>
  
#### Quality control of raw sequencing data (FastQC)

  * During each sequencing cycle, Illumina generates a BCL basecall file, which is then converted to a FASTQ file for subsequent analysis.
  * The FASTQ file contains information about the sequencer and the flow cell (e.g., flow cell ID, X and Y coordinates of the sample, etc.), the read sequence, a "+" sign, and quality information denoting the probability of an incorrect base call at a given position. 
  * An example FASTQ file is given below, detailing the various metadata in the FASTQ file header (image courtesy of [databricks](https://databricks.com/blog/2016/05/24/parallelizing-genome-variant-analysis.html))

<br>
<p align="center">
<img src="https://databricks.com/wp-content/uploads/2016/05/FASTQ-format-1024x202.png" alt="Example FASTQ file.">
</p>
<br>

  * FastQC is a tool for evaluating the quality of sequencing data, giving the user basic statistics such as the distribution of quality scores across the length of the reds, per base N content (i.e., no call), and the number and identity of overrepresented sequences that could be caused by contamination from the library preparation process (e.g., adapter dimer contamination).

#### Quantify expression

  * After evaluating the quality of sequence data and identifying any potential issues that could cause problems downstream, the next step is to quantify the expression of each transcript.
  * In other words, the goal is to identify which genes (or targets, more generally), each read is derived from.
  * Splice-aware alignment software exists to align RNA-seq reads to a reference genome (e.g., HISAT2 and STAR), as RNA-seq reads may span exon-exon junctions that cannot be mapped using short-read aligners common in DNA sequencing approaches.
  * Other alignment tools are so called "lightweight" in that they don't rely on direct alignment to a reference genome- some of the popular software programs in this cateogry are Salmon, Kallisto, and Sailfish.
  * [Salmon](https://combine-lab.github.io/salmon/) works by quasi-mapping RNA-seq reads to a reference transcriptome, essentially bypassing the computation of trying to align reads to each of several possible mapping positions in a reference genome and quantifying how well it matches to each position; instead, it just generates counts directly from the reference transcriptome.
  * [Sailfish](http://www.cs.cmu.edu/~ckingsf/software/sailfish/) uses a similar approach, instead using k-mers and a set of reference sequences (e.g., a reference transcriptome) to generate abundance estimates without aligning to a reference genome.
  * [Kallisto](https://pachterlab.github.io/kallisto/about) uses an approach called pseudoalignment, in which only the transcripts from which RNA-seq reads would have originated from are considered, without regard for how the reads and reference transcriptome align to generate abundance estimates.

####Quality control of aligned sequence reads (STAR/Qualimap)

  * Although for this tutorial we'll be using the abundance estimates generated from Salmon, it is still important to evaluate how well the RNA-seq reads map to a reference genome.
  * This can be accomplished using splice-aware aligners like STAR or HiSAT2, both of which can generate binary alignment map (BAM) files.
  * BAM files are the compressed version of the sequence alignment/map format file (SAM), which describes the alignment of reads to a reference with two sections: the header section, which contains metadata like the read name, read length, and the alignment method; and an alignment section, with data on the read name, sequence, quality, and additioanl alignment information (e.g., chromosome, start coordinate, and alignment quality).
  * We can use the tool Qualimap to provide summary information for each BAM file, similar to how FastQC is used to evaluate potential issues with sequencing data.
  * For example, a screenshot is given below demonstrating how Qualimap can be used to evaluate the distribution of coverage across a reference for a given BAM file.

<br>
<p align="center">
<img src="http://qualimap.conesalab.org/img/screenshot_bamqc_coverage.png" alt="Example screenshot of the coverage across reference using Qualimap">
</p>
<br>

#### Quality control: aggregating results with MultiQC 

  * [MultiQC](https://multiqc.info/) is a program that can be used to aggregate the results from various bioinformatics QC analyses (e.g., FastQC, Qualimap, alignment log files, etc.) for each sample into a single HTML report.
  * Briefly, the user will run MultiQC in an analysis directory containing all of the QC and log files and generate a single HTML report with all samples aggregated together to identify any patterns or outliers that may warrant investgiation before proceeding with subsequent analyses.
  * An example screenshot from a MultiQC HTML report shows an  plot where the user can interactively investigate the GC content per sequence. 

<br>
<p align="center">
<img src="https://multiqc.info/docs/images/toolbox_highlight.png" alt="Example screenshot of a MultiQC HTML report showing an interactive plot of the GC content per sequence.">
</p>
<br>

### 2. Experimental design considerations

This section highlights factors to consider when designing RNA-seq experiments, particularly with regard to replicates and achieving the desired sequencing depth and coverage for the desired anlyses.

Some notes for each section are presented below:

#### Replicates

  * Technical replicates are replicate samples obtained from the same specimen to measure technical variation associated with a particular process (e.g., taking three DNA samples from the same mouse to measure technical variation in an assay).
  
  * Biological replicates are different samples taken from the same condition with the goal of measuring biological variability (e.g., taking three DNA samples from separate mice to measure biological variation between samples with a given assay).
  
  * In this tutorial, the authors suggest that technical replicates are not necessary, but biological replicates are necessary in order to detect meaningful differences in the gene expression between treatments. 
  
  * Further, when considering trade-offs between sequencing depth and biological replicates, the authors demonstrate with published data that biological replicates will identify more differentially expressed genes relative to simply increasing the sequencing depth (i.e., the number of reads per sample).
  
  * However, there are other factors to consider when designing the experiment- e.g., if a goal is to measure lowly expression differentially expressed genes, generally a greater sequencing depth is needed than would be recommended for a more general approach.
  
  * Coverage, a term used to describe how many times (on average) each position in a genome has been sequenced, is not used in RNA-seq experiments because gene expression is highly variable.  
  
#### Confounding

  * Confounding refers to situations in which a researcher cannot distinguish between biological variation and technical variation (e.g., all male mice were used in a control group and all female mice were used in an experimental group).
  
#### Batch effects 

  * Batch effects refer to situations where you have additional confounding variables related to components of the workflow- for example, sequencing all samples from a particular species on one flow cell, and all samples from another species on a different flow cell, when trying to compare transcriptional activity at the same genes across species.
  
  * Gilan and Mizrahi-Man (2015; F1000 Research) highlighted that when reviewing mouse ENCODE comparative gene expression data between mouse and human, data tended to cluster more by species than by tissue type- however, it was noted that each species was sequenced separately, thus introducing bias in the form of a batch effect.  Below is a figure showing a principal components analysis and heatmap using hierchical clustering of the original data.
  
<p align="center">
<img src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4516019/bin/f1000research-4-7019-g0001.jpg"
alt = "PCA and heatmap of original comparative gene expression data between mouse and human from Gilan and Mizrahi-Man 2015 demonstrating the batch effect.">
</p>

  * After accounting for the batch effect (introduced by sequencing human and mouse tissue samples on the same sequencing lane as their respective species), the data clustered by tissue as expected.  Below is a figure showing the same analyses with the data after accounting for the batch effect.
  
<p align="center">
<img src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4516019/bin/f1000research-4-7019-g0002.jpg"
alt="PCA and heatmap of comparative gene expression data between mouse and human, after accounting for batch effects, from Gilan and Mizrahi-Man 2015.">
</p>

  * The authors recommend that if batch effects can't be avoided, researchers should exercise caution to avoid confounding in experimental group allocation.
  
  * Additionally, it is recommended to include the batch ID (e.g., a numeric value) in experimental metadata so that during normal QC steps, if potential batch effects are identified, they can be accounted for with linear regression or similar techniques.  
  
#### Exercise

1) Fill in a group value in the `rna_isolation` field in the Excel file from the training GitHub page (linked [here](https://hbctraining.github.io/DGE_workshop_salmon_online/lessons/experimental_planning_considerations.html)).  The idea is to: a) randomly allocate group treatment for RNA isolation; and b) give equal numbers of RNA isolations for each researcher.  For both of these tasks, we'll create a vector of possible assignments (e.g., `group1` through `group6`), repeat twice (there are 12 total samples), and then use `sample()` to create a vector of randomly drawn group assignments that we can use in the `rna_isolation` field, for example.

```{r part1-ex1}
# Create data directory
dir.create("./metadata")

# The file was downloaded manually from GitHub and moved into the metadata directory

# Import the Excel file and use the janitor package to tidy the field names
exp_design_table <- read_excel("./metadata/exp_design_table.xlsx") %>% 
  clean_names()

# Now we want to allocate the group assignments
# We'll define a vector of groups from 1 to 6, repeated twice, and then use sample() to generate a random assignment to allocate different samples to rna_isolation groups
groups <- c(paste("group", 1:6, sep = "")) %>% 
  rep(2) %>% 
  sample(size = 12, replace = FALSE)

# Now we'll do the same thing to assign rna_isolations for each researcher
researcher <- c("AB", "CD") %>% 
  rep(6) %>% 
  sample(size = 12, replace = FALSE)

# Add the random group and researcher assignments to the data frame
exp_design_table <- exp_design_table %>% 
  mutate(rna_isolation = groups, 
         researcher = researcher)
  
# Print out the results
exp_design_table %>% 
  kbl(caption = "Experimental design table after random assignment to one of six RNA isolation groups and one of two researchers to control for batch effects.") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

### 3. Intro to DGE/setting up DGE analysis

This section describes the basic workflow of an RNA-seq experiment, the objectives and motivations behind the experiment, and how to set up analyses to evaluate differences in gene expression.  

Notes for each section are presented below:

#### Different gene expression analysis

  * The basic workflow in this notebook is to take the pseudocounts generated from the different lightweight alignment programs (e.g., Salmon) and convert them to counts of genes, perform QC on these data through normalization and unsupervised clustering to detect potential batch effects (e.g., principal components analysis; PCA), and then perform the differential expression analyses.  
  
#### Review of the dataset

  * The dataset that will be used in this tutorial is from Kenny et al. 2014 (found [here](https://pubmed.ncbi.nlm.nih.gov/25464849/)) in which the authors compare the expression of MOV10 in three different groups: MOV10 over-expression, in which HEK293F (human epithelial kidney cells) were transfected with a MOV10 transgene; MOV10 knockdown, in which a short interfering RNA (siRNA) was used to knock out MOV10 expression; and in which siRNA created a MOV10 knockdown via non-specific knock out.  
  
  * Importantly, each replicate has three replicates.
  
  * MOV10 associates with the fragile X mental retardation protein (FMRP), a protein responsible for normal cognitive development, but which is absent in the brain of those with Fragile X syndrome.
  
  * The authors hypothesized that MOV10 and FMRP act in coordination to suppress translation of bound RNAs, which may be implicated in the disease. 
  
  * In this analysis, we are mainly trying to identify patterns gene expression depending on the gain or loss of function of MOV10, and if there are genes with similar expression patterns shared between treatments. 
  
  * The data for the analysis were downloaded from the Sequence Read Archive (SRA) and processed using the steps defined in the above sections.
  
#### Setting up

To set up for the differential expression analyses, we need to download the Salmon results from the full dataset from [this](https://hbctraining.github.io/DGE_workshop_salmon_online/lessons/01b_DGE_setup_and_overview.html) page by right-clicking and saving the zipped directory directly into the project directory.  We can then unzip the directory, which will create a new `data` directory with subdirectories for each sample.  Next, we'll need to download the annotation file to map the transcripts to the genes.  We can download this text file from of the HBC's GitHub repositories [here](https://github.com/hbctraining/DGE_workshop_salmon/blob/master/data/tx2gene_grch38_ens94.txt).

```{r salmon-data-import}
# Unzip the data.zip directory
unzip("./data.zip")

# Remove the unwanted directories and the old data.zip directory
unlink(c("__MACOSX/", "data.zip"),
       recursive = TRUE)

# Download the annotation file from the GitHub repository
download.file("https://raw.githubusercontent.com/hbctraining/DGE_workshop_salmon_online/master/data/tx2gene_grch38_ens94.txt", 
              destfile = "./metadata/tx2gene_grch28_ens94.txt")
```

Now that we've downloaded all of the Salmon output files and the annotation file, we need to load the libraries from CRAN and Bioconductor (see setup code chunk at the beginning of this document).  Note, any Bioconductor packages need to be installed using the command `Biocmanager::install(<packagename>)`. 

#### Loading data

Initially, we will be working with the `quant.sf` Salmon output files (detailed information regarding the file formats and other documentation can be found [here](https://salmon.readthedocs.io/en/latest/file_formats.html).  An example of one of the files is shown below.

```{r salmon-quantsf-example}
# Print an example of the Salmon quant.sf output file
read_tsv("./data/Mov10_oe_1.salmon/quant.sf") %>% 
  head() %>% 
  kbl(caption = "Example quant.sf Salmon output for one of the MOV10 over-expression replicate RNA-seq libraries") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

```

The `quant.sf` output is a tab-separated quantification file, where each row is a quantification record containing the following fields:

* `Name`: contains the name of the transcript from the database.  For example, in the table above, ENST00000456328 is DEAD/H-box helicase 11 like 1 (more information on this particular transcript found [here](http://m.ensembl.org/Homo_sapiens/Transcript/Summary?db=core;g=ENSG00000223972;r=1:11869-14409;t=ENST00000456328)).

* `Length`: length of the transcript, in nucleotides.

* `EffectiveLength`: the effective length of the transcript, which takes into account the different factors being modeled like GC-content and the distribution of the fragment lengths from the underlying transcript that will influence the probability of sampling fragments from this target transcript.  

* `TPM`: transcripts per million, a relative metric of transcript abundance.

* `NumReads`: Salmon's estimate of the number of reads mapping to the particular transcript.  

In order to get these data in the right format for analysis with DESeq2, we need to convert the normalized TPM counts to non-normalized count estimates.  Additionally, we need to collapse our read counts per transcript to read counts per gene, since there are potentially several target transcripts per gene.  The first thing we need to do is name all of the `quant.sf` files to discriminate between samples in downstream analyses.  We'll first create two variables: one variable defining the paths for all directories containing data (there will be nine- one for each sample); and another vector containing the filenames for all of the `quant.sf` files.  We'll use these vectors to create unique names for each `quant.sf` file.

```{r rename-quant-files}
# Create a vector of the paths to the sample-specific directories containing data
samples <- list.files(path = "./data", 
                      full.names = T, 
                      pattern = "salmon$") # define the names of the directories as anything that ends with "salmon"

# Check to see that the list of directories is correct
samples # All of the sample-specific directories are present

# Create a vector of all of the quant.sf file names for each sample-specific directory
files <- file.path(samples, "quant.sf")

# Check to see that the paths to all of the quant.sf files are correct
files # All of the quant.sf file paths are correct

# Create unique names for each of the directories 
names(files) <- str_replace(samples, "./data/", "") %>% 
  str_replace(".salmon", "")

# The above code assigns names to each of the elements in the files vector
# First, it takes the sample-specific directory names and removes the "./data/" prefix, then removes the ".salmon" extension to create a unique name that can be referred to easily in downstream analyses
```

Now, we'll need to import our annotation file that contains the reference information linking the transcript IDs that were used to estimate read counts with Salmon to the genes that the transcripts are derived from.

```{r import-annotation-file}
# Import the annotation file
tx2gene <- read_tsv("./metadata/tx2gene_grch28_ens94.txt")

# Take a look at the annotation file
tx2gene %>%
  head() %>% 
  kbl(caption = "Annotation file with reference information linking the transcript ID used in the Salmon index to its gene ID and symbol") %>%
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()
  
```

The first column is the transcript ID, the second column is the gene ID, and the third ID is the gene symbol. Now we'll need to run the `tximport()` function, which imports the transcript-level estimates from different programs (e.g., Salmon) and summarizes abundances to the gene level using the provided annotation file.  There are several arguments: `files`, which we'll define using the `files` vector containing all of the directories to the `quant.sf` files (defined above); `type`, to define which program was used to generate the counts; `tx2gene`, which specifies the two-column data frame linking the transcript ID to the gene ID (i.e., the first two columns in the annotation file); and `countsFromAbundance`, which we'll set to `lengthScaledTPM` to we use the counts from the `TPM` field in the `quant.sf` file, because we don't want to use the counts that are correlated with the effective length.

```{r run-tximport}
# Run tximport for all files
txi <- tximport(files = files, 
                type = "salmon", 
                tx2gene = select(tx2gene, c("tx_id", "ensgene")),
                countsFromAbundance = "lengthScaledTPM")

```

#### Viewing data

The resulting `txi` object is a list of various matrices: `abundance`, `counts`, `length`, and `countsFromAbundance` (this is just the character given in the `countsFromAbundance` argument in the function call).    

```{r txi-attributes}
attributes(txi)
```

Now we want to look specifically at the `counts` matrix, which we'll round to the nearest whole number and then convert to a data frame.

```{r txi-counts-dataframe}
# Round the txi counts data and conver the matrix to a data frame
counts_data <- txi$counts %>% 
  round() %>% 
  data.frame() %>% 
  rownames_to_column("Gene")

# Take a look at the data frame
counts_data %>% 
  head() %>% 
  kbl(caption = "Count data for each gene obtained from Salmon pseudocounts") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

# Save this counts table to a file in the data directory
write_csv(counts_data, 
          file = "./data/counts_data.csv")
```

#### Creating metadata

Now we just want to create a metadata file linking the headers from the `txi$count` matrix to the sample groups (i.e., control, overexpression, and knockdown).  The Salmon data file that was downloaded from the tutorial webpage actually only had two MOV10 knockdown replicates, so we will need to change the metadata data frame accordingly.  

```{r}
# Create a metadata table
counts_metadata <- data.frame(sample_type = c(rep("control", 3), 
                           rep("MOV10_knockdown", 2), 
                           rep("MOV10_overexpression", 3)), 
           sample_name = colnames(txi$counts))

# View the metadata
counts_metadata %>% 
  kbl(caption = "Metadata linking the sample type to the sample name in the gene counts table") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()

# Write this data frame to a file in the metadata directory
write_csv(counts_metadata, 
          file = "./metadata/counts_metadata.csv")
```

## Part II: QC and setting up for DESeq2

### 1. RNA-seq counts distribution

#### Explore RNA-seq count data

 Below is the counts data frame with the genes in the first column and the samples in the following columns.  Each cell contains an integer which represents the number of transcripts from a particular gene in that sample.
 
```{r visualize-count-data}
counts_data %>% 
  head() %>% 
  kbl(caption = "Count data for each gene obtained from Salmon pseudocounts") %>% 
  kable_styling(full_width = FALSE) %>% 
  kable_classic_2()
```
 
We can plot the distribution of counts for each sample using a histogram, which will demonstrate the common features of RNA-seq data: there are generally low counts for a large proportion of the genes, and a large tail in the distribution as there is no upper limit to gene expression.

```{r counts-histogram}
# Generate a histogram for each sample
counts_data %>% 
  pivot_longer(cols = !Gene, 
               names_to = "Sample", 
               values_to = "Count") %>% 
  ggplot(aes(Count)) +
  geom_histogram(stat = "bin", 
                 bins = 200) +
  facet_wrap(~ Sample) +
  my_theme

```

#### Modeling count data

* Count data can be modeled either with the binomial distribution (e.g., modeling the number of times you would expect to see heads in a series of coin tosses) or the Poisson distribution.  The latter if often used when the number of trials is very large, but assumes that the mean and the variance are equal.  

* To evaluate whether we meet this assumption, we'll use the three MOV10 over-expression replicates and generate vectors for the mean counts for each gene and the variance.  

* Then, we can visualize the relationship between the mean and variance in a scatterplot.

```{r mean-variance-mov10-overexpression, warning = FALSE, message = FALSE}
# Calculate mean and variance for each gene in the three Mov10 overexpression samples
counts_data_stats <- counts_data %>% 
  select(Gene, contains("Mov10_oe")) %>% 
  mutate(Mean_count = apply(select(., contains("Mov10_oe")), 1, mean), 
         Variance = apply(select(., contains("Mov10_oe")), 1, var))

# Plot the mean and variance and add a continuous slope line where mean = variance
counts_data_stats %>% 
  ggplot(aes(Mean_count, Variance)) +
  geom_point(alpha = 0.5) +
  scale_x_log10(limits = c(1, 1e9)) +
  scale_y_log10(limits = c(1, 1e9)) +
  geom_abline(intercept = 0, 
              slope = 1, 
              linetype = "dashed", 
              color = "red") +
  my_theme +
  labs(x = "Mean counts", 
       y = "Variance counts", 
       title = "Mean versus variance in RNA-seq count data in MOV10 over-expression samples")
```

* This plot reveals that the mean and variance are not equal- if they were, the points would be right along the dashed red line.

* Instead, we can see that for genes with higher mean counts, the variance is higher than the mean (i.e., the points are above the dashed line).

* The genes with lower mean counts also display heteroscedasticity, or the tendency for low mean counts to have high variability in the variance estimates.

#### An alternative: the negative binomial distribution

* [This blogpost](http://bridgeslab.sph.umich.edu/posts/why-do-we-use-the-negative-binomial-distribution-for-rnaseq) provides a brief explanation for using the negative binomial distribution to model count data in RNA-seq data sets.

* In summary, RNA-seq data cannot be normally distributed, because we cannot have negative counts, or fractions of counts; all of the counts must be positive whole numbers (i.e., positive integers).  

* Because the Poisson distribution assumes the mean and variance are equal, this is not appropriate for RNA-seq data sets where the number of replicates is relatively small and thus, we cannot accurately estimate variance.

* The negative binomial distribution is an appropriate alternative, as this probability distribution doesn't assume equal mean and variance for count data. 

* However, if we had enough replicates (which is often unfeasible for RNA-seq experiments), we may be able to use the Poisson if the variance is reduced such that the assumption of equal mean and variance is satisfied.

#### Exercise

1) If we observed little to no variability between samples, this might suggest that we can model the RNA-seq count data with the Poisson distribution where mean and variance are equal.  

2) We would expect the mean and variance to be equal for this dataset.  

#### Replicates and variability

* Recall, biological replicates come from the same biological condition but different sources (e.g., DNA samples taken from mice raised in the same experimental conditions).

* One of the challenges in differential gene expression analysis is identifying and correcting for unknown sources of biological (e.g., mice with different genetic backgrounds) or technical variation (e.g., different lot numbers of reagents, etc.).

* HBC recommends running running experiments in triplicate (e.g., three replicates for a experimental group and three replicates for a control group).

* However, more replicates are generally always better because we can get more precise estimates of the mean and variance, and thus, better detect meaningful differences between groups.

* The authors also include a figure from a study showing the increase in the number of differentially expressed genes as a function of the number of replicates included in a study, more so than the sequencing depth.  

#### Differential Expression with DESeq2

* We will be using the DESeq2 package for differential gene expression analysis, which employs the negative binomial distribution to model counts from RNA-seq data.

### 2. Count normalization

#### Normalization

* Normalization involves adjusting the counts for each gene and sample to account for the variability associated with factors that aren't related to the experimental differences between groups, for example.

* There are several factors which need to be considered during the normalization step:

  * __Sequencing depth:__ we need to account for differences in the number of reads which map to each gene (sequencing depth), as we may observe differences in counts that are simply due to differences in sequencing depth.
  
  * __Gene length:__ we also need account for differences in counts between different genes in the same sample, because one gene may be longer than the other, and longer genes have a biased increase in the number of reads which map to them relative to shorter genes.
  
  * __RNA composition:__ some genes may have large differences in counts between two samples, and because of that, may dominate the differential expression signal and drown out lower expressed genes that are differentially expressed if they are not normalized to adjust for these large differences. 
  
#### Normalization methods

* There are different normalization methods used, most of which account for either sequencing depth or a combination of sequencing depth and gene length, but are not suitable for evaluating between-sample comparisons or differential gene expression analyses (e.g., transcripts per million).

* The reason we can't use some normalization methods like reads per kilobase per million mapped reads (RPKM) is because the RPKM-normalized counts between samples could be different, which doesn't allow us to evaluate differences in gene expression.

#### DESeq2-normalized counts: median of ratios method

* Differences in sequencing depth (differences in the number of reads mapped to the same gene in two different samples) and the RNA composition need to be accounted for in the normalization method.

* DESeq2 uses the median of ratios method to normalize counts, which involves several steps:

1) Create a pseudo-reference sample by calculating the row-wise geometric mean.
  * The geometric mean is calculated by taking the square root of the product of counts row-wise, and is used in situations when two counts may be very different and is less sensitive to influence from the larger number.
  
2) Calculate the ratio of the count for each sample to the pseudo-reference sample.

3) Calculate the normalization factor for each sample by taking the median of all of the ratios for each sample to the pseudo-reference across all genes.  

  * The key concept behind this approach is that it assumes that not all of the genes are differentially expressed, so it successfully accounts for the sequencing depth and RNA composition because it is not sensitive to larger outlier genes.  
  
4) Calculate the normalized counts for each gene in each sample by dividing the raw counts by the per-sample normalization factor.  

* Note, the normalized counts will not necessasrily be whole numbers.  

#### Count normalization of Mov10 dataset using DESeq2

* First, we need to make sure that the counts data and the metadata have matching information.

```{r match-metadata-counts-data}
# Match the metadata and counts data files
all(colnames(txi$counts) %in% counts_metadata$sample_name) # TRUE
all(colnames(txi$counts) == counts_metadata$sample_name) # TRUE
```

#### Exercises

* If the sample names in the counts matrix and the metadata file are the same, but they are in a different order, how can we re-order them to satisfy this requirement?

```{r reordering-names}
# Test vectors
a <- c("apple", "banana", "orange")
b <- c("banana", "apple", "orange")

# Test conditions
all(a %in% b) # TRUE
all(a == b) # FALSE- because they are in a different order

# Try rearranging with match using bracket subsetting
c <- a[match(a, b)]

# Test again
all(c %in% b) # TRUE
all(c == b) # TRUE

# Reverse the sample name order
counts_metadata_testing <- counts_metadata %>% 
  arrange(desc(sample_name))

# Test again with the counts matrix
all(colnames(txi$counts) %in% counts_metadata_testing$sample_name) # TRUE
all(colnames(txi$counts) == counts_metadata_testing$sample_name) # FALSE

# Apply the line of code to fix the names
new_arrangement <- counts_metadata_testing$sample_name[match(counts_metadata_testing$sample_name, colnames(txi$counts))]

# Re-arrange the counts_metadata_testing data frame with the new arragement
counts_metadata_testing <- counts_metadata_testing %>% 
  arrange(match(.$sample_name, new_arrangement))

# Test one last time 
all(colnames(txi$counts) %in% counts_metadata_testing$sample_name) # TRUE
all(colnames(txi$counts) == counts_metadata_testing$sample_name) # TRUE
```

* To create a DESeq2 object, we need both the count matrix and metadata (which is why they need to be in the same order) as well as a design formula, which controls how the package evaluates gene expression across different groups (e.g., control, knockdown, and over-expression).

```{r create-DESeq2-object}
# Convert the characters in the metadata to factors
counts_metadata <- counts_metadata %>% 
  mutate(across(where(is.character), as.factor))

# Create the DESeq2 object by specifying the metadata as the colData and the sample type as the design
dds <- DESeqDataSetFromTximport(txi, colData = counts_metadata, design = ~sample_type)

```

* The next step is to perform the median of ratios normalization step using the `estimateSizeFactors()` function.

```{r normalize-counts}
# Normalize counts using median of ratios method
dds <- estimateSizeFactors(dds)

# View the sample-specific normalization factors using the sizeFactors() function
data.frame(norm_factor = sizeFactors(dds)) %>% 
  rownames_to_column("sample") %>% 
  kbl(caption = "Per-sample median of ratios normalization factors calculated with DESeq2") %>% 
  kable_styling(bootstrap_options = "striped", 
                full_width = FALSE) %>% 
  kable_classic_2()
```
* Now to obtain the normalized size factors, we use the `counts()` function with the argument `normalized = TRUE`.  

```{r extract-normalized-counts}
# Extract the normalized counts from the DESeq2 data set
normalized_counts <- counts(dds, normalized = TRUE)

# Save this as a text file for later 
normalized_counts %>% 
  data.frame() %>% 
  rownames_to_column("Gene") %>% 
  write_csv(., "./data/normalized_counts.csv")

# View the first few normalized counts to see how they've changed from the original counts
t1 <- counts(dds) %>% 
  head() %>% 
  data.frame() %>% 
  rownames_to_column("Gene")

t2 <- normalized_counts %>% 
  head() %>% 
  data.frame() %>% 
  rownames_to_column("Gene") %>% 
  mutate(across(where(is.numeric), ~round(.x, digits = 2)))

kable(list(t1 %>% select(., Gene, contains("Mov10_oe")), 
           t2 %>% select(., Gene, contains("Mov10_oe"))), 
      caption = "Raw (left) versus normalized (right, rounded to two digits) gene counts from DESeq2", 
      booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE, 
                bootstrap_options = "striped") %>% 
  kable_classic_2()
```

### 3. Sample-level QC

#### Sample-level QC

* The basics of QC for RNA-seq data is to evaluate our data at the sample level and at the gene level.

* For example, one of the questions we want to know is how samples cluster together or appear more similar, and if this matches what we would expect based on our background knowledge.

* For sample-level QC, we really want to know how similar the replicates are to each other, mostly to confirm that the experimental condition (e.g., treatment) is the major source of variation in the data, not sample-to-sample variability.

* Instead of just using the normalized counts, we'll use the regularized log-transformed normalized counts, which helps to avoid the high variability in variance estimates in genes with low counts (see above sections) by shrinking those low read counts toward the per-gene average across all samples.

* The genes with high counts will have transformed values similar to those obtained using the log2 transformation.

#### Principal components analysis (PCA)

* PCA is an powerful tool for dimensionality reduction and in this context, if replicates have similar expression profiles across genes, they will cluster together in PCA space relative to samples from a different treatment condition with different expression profiles.

* In some cases, as in the example in the tutorial, the treatment is not the only source of variation in the data- in fact, other factors like sex may contribute a larger amount to the overall variability in the data instead of the treatment. 

* However, in some cases, it may be possible to account for these factors in the modeling step such we can still parse out the differentially expressed genes associated with the treatment. 

#### Hierarchical clustering heatmap

* The heatmap shows the correlation of gene expression for all pairwise combinations of samples in the data and the different axes display substructure in the data.

* In the example in the tutorial, we would expect that the sample replicates would have a high correlation in their gene expression profiles, and would thus cluster together in the heatmap.

#### MOV10 quality assessment and exploratory analysis using DESeq2

* The first step to quality assessment with PCA and hierarchical clustering is to transform the normalized counts using the regularized-log2 transformation to account for the heteroskedastic nature of the data (i.e., variance in the response is not equal across the range of the predictor values) and normalize by library size.

```{r rlog-transformation}
# Transform the normalized counts using rlog2 transformation
# Importantly, we need to use the `blind=TRUE` argument so the transformation is done without taking into account the sample groups
rld <- rlog(dds, blind = TRUE)

```

#### PCA for the MOV10 dataset

* The `DESeq2::plotPCA()` function plots the PCA using `ggplot2` under the hood and takes the `rlog()` transformed object directly as input.

```{r mov10-pca}
# Generate a PCA plot for the MOV10 data
plotPCA(rld, intgroup = "sample_type") +
  labs(title = "QC of MOV10 gene expression data", 
       subtitle = "Normalized gene counts were transformed using the regularized log2 transformation") +
  my_theme
```

#### Exercise 

1) _What does the above plot tell you about the similarity of samples?_

The plot indicates that each biological replicate is more similar to one another than to other biological replicates.  

2) _Does it fit the expectation from the experimental design?_

The experimental design includes three biological replicates for each group (except for one group): three control replicates, two MOV10 knockdown replicates, and three MOV10 over-expression replicates.  From this design, we could expect all of the controls to group together, all of the knockdowns to group together, and all of the over-expression replicates to group together.  Indeed, this is exactly what we see.  

3) _What do you think the % variance information (in the axes titles) tell you about the data in the context of the PCA?_

The `% variance` on each axis describes the variation in the gene count data explained on each principal component axis.  The strategy behind PCA is to reduce dimensionality and explain the total variation in the data set with as few variables as possible.  In the context of the experiment, the first principal component separates the MOV10 treatments: the knockdowns and the over-expression replicates.  The second principal component separates the MOV10 treatments from the control treatmet with normal MOV10 expression.

#### Additional PCA QC

* While the `plotPCA()` function is a powerful tool to quickly perform quality control on RNA-seq data, it has two caveats: it will only plot the top 500 most variable genes (which likely explains more than enough variation anyway), and it will only plot the first two principal components.  For additional functionality to plot different principal components, we can use the `prcomp()` function.  This will also allow us to extract useful information like the loadings (stored in the `rotation` slot), or the "weight" of different genes in explaining variation along a PCA axis.  

* Importantly, the matrix gives the observations as columns and the features (i.e., genes) as rows, but we need this to be flipped so the observations are in rows and the variables in columns.  See this stacks exchange [post](https://stats.stackexchange.com/questions/248825/why-do-we-need-to-take-the-transpose-of-the-data-for-pca) for more details.

```{r mov10-pca-custom}
# Extract the regularized log-transformed counts into a matrix
rld_pca <- assay(rld) %>% 
  t() %>% 
  prcomp()

# Extract summary information
summary_df <- summary(rld_pca) %>% 
  pluck("importance") %>% 
  data.frame() %>% 
  rownames_to_column("Term") %>% 
  filter(Term == "Proportion of Variance") %>% 
  mutate(across(where(is.numeric), function(x) round(x * 100, digits = 2)))
  
# Import metadata
metadata <- read_csv("./metadata/counts_metadata.csv")

# Extract coordinates from PCA into a data frame and bind with metadata
pca_df <- cbind(metadata, rld_pca$x)

# Create plotting function
plot_pca <- function(df, axis1, axis2) {
  df %>% 
    ggplot(aes_string(paste("PC", axis1, sep = ""), 
               paste("PC", axis2, sep = ""))) + 
    geom_point(aes(color = sample_type), 
               size = 3) +
    labs(x = paste("PC", axis1, ": ", eval(summary_df %>% dplyr::select(paste("PC", axis1, sep = ""))), "%", sep = ""), 
         y = paste("PC", axis2, ": ", eval(summary_df %>% dplyr::select(paste("PC", axis2, sep = ""))), "%", sep = "")) +
    my_theme +
    theme(
      axis.title.x = element_text(face = "bold"), 
      axis.title.y = element_text(face = "bold")
    )
}

plot_pca(pca_df, 1, 2) + 
  plot_pca(pca_df, 1, 3) + 
  plot_pca(pca_df, 2, 3) +
  plot_layout(guides = "collect") +
  plot_annotation(tag_levels = "A", tag_suffix = ")") &
  theme(legend.position = "bottom")
```

* We can see additional patterns arise when we plot different PC axes.  Notably, when we plot PC1 and PC3, the MOV10 knockdown and control samples cluster together.  Interestingly, there is some odd behavior with one of the MOV10 over-expression samples on PC3 that may need to be investigated further.

#### Hierarchical Clustering for the MOV10 dataset

* To construct a heatmap, we need to use the `SummarizedExperiment::assay()` function to extract a matrix from the `DESeq2` object, then calculate the correlation using the `cor()` base R function to calculate all of the pairwise correlations for all samples.  We'll use the `metadata` table to annotate the heatmap.

```{r mov10-heatmap}
# Extract the matrix from the DESeq2 object
assay_mat <- assay(rld) %>% 
  cor()

# Create new metadata object
metadata <- data.frame(sample_type = rep(c("control", "knockdown", "overexpression"), 
                                    c(3, 2, 3)))
row.names(metadata) <- colnames(assay_mat)

# Plot the heatmap
pheatmap(assay_mat, annotation = metadata)
```

* Overall, we see high correlations between samples of the same type, indicating that this dataset won't likely lead to any issues when identifying differentially expressed genes.  

### 4. Design formulas

### 5. Hypothesis testing and multiple test correction

## Part III: DESeq2

### 1. Description of steps for DESeq2

### 2. Wald test results

### 3. Summarizing results and extracting significant gene lists

### 4. Visualization

### 5. Likelihood Ratio Test Results

### 6. Time Course Analysis

## Part IV: Functional Analysis

### 1. Gene Annotation

### 2. Functional Analysis I: Over-Representation Analysis

### 3. Functional Analysis II: Functional Class Scoring/GSEA  

